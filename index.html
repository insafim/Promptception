<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Promptception: How Sensitive Are Large Multimodal Models to Prompts?">
  <meta name="keywords" content="Promptception, LMMs, Prompt Sensitivity, EMNLP 2025, MBZUAI">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Promptception: How Sensitive Are Large Multimodal Models to Prompts?</title>

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma@0.9.1/css/bulma.min.css">
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/Promptception-Logo.png">
  <link href="https://fonts.googleapis.com/icon?family=Material+Icons" rel="stylesheet">


  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/js/all.min.js"></script>
  <script type="module" src="https://gradio.s3-us-west-2.amazonaws.com/3.27.0/gradio.js"></script>
</head>


<style>
    .section {
    margin-bottom: -30px; /* Adjust this value as needed to reduce the space */
  }
  .expandable-card .card-text-container {
    max-height: 200px;
    overflow-y: hidden;
    position: relative;
  }

  .expandable-card.expanded .card-text-container {
    max-height: none;
  }

  .expand-btn {
    position: relative;
    display: none;
    background-color: rgba(255, 255, 255, 0.8);
    /* margin-top: -20px; */
    /* justify-content: center; */
    color: #510c75;
    border-color: transparent;
  }

  .expand-btn:hover {
    background-color: rgba(200, 200, 200, 0.8);
    text-decoration: none;
    border-color: transparent;
    color: #510c75;
  }

  .expand-btn:focus {
    outline: none;
    text-decoration: none;
  }

  .expandable-card:not(.expanded) .card-text-container:after {
    content: "";
    position: absolute;
    bottom: 0;
    left: 0;
    width: 100%;
    height: 90px;
    background: linear-gradient(rgba(255, 255, 255, 0.2), rgba(255, 255, 255, 1));
  }

  .expandable-card:not(.expanded) .expand-btn {
    margin-top: -40px;
  }

  .card-body {
    padding-bottom: 5px;
  }

  .vertical-flex-layout {
    justify-content: center;
    align-items: center;
    height: 100%;
    display: flex;
    flex-direction: column;
    gap: 5px;
  }

  .figure-img {
    max-width: 100%;
    height: auto;
  }

  .adjustable-font-size {
    font-size: calc(0.5rem + 2vw);
  }

  .chat-history {
    flex-grow: 1;
    overflow-y: auto;
    /* overflow-x: hidden; */
    padding: 5px;
    border-bottom: 1px solid #ccc;
    margin-bottom: 10px;
  }

  #gradio pre {
    background-color: transparent;
  }
</style>

<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <img src="static/images/Promptception-Logo.png" alt="Promptception" width="100">
            <!-- <h1 class="title is-1 publication-title">Promptception: How Sensitive Are Large Multimodal Models to Prompts? <br> <span style="color: red; font-size: 0.7em;">[EMNLP 2025]</span> </h1> -->
            <h1 class="title is-1 publication-title">Promptception: How Sensitive Are Large Multimodal Models to Prompts? <br> <span style="color: gray; font-size: 0.6em;">[EMNLP 2025 Findings]</span> </h1>
            <div class="is-size-5 publication-authors">
            <!-- <img src="static/images/emnlp_2025_logo_v1.png" alt="EMNLP_logo" width="100"> -->
              <!-- First Group of 3 Authors -->
              <div class="author-group">
                  <span class="author-block">
                      <a href="https://scholar.google.com/citations?user=--fYSbUAAAAJ&hl=en" style="color:#f68946;font-weight:normal;">Mohamed Insaf Ismithdeen,
                  </span>
                  <span class="author-block">
                      <a href="https://scholar.google.com/citations?user=M6fFL4gAAAAJ&hl=en" style="color:#008AD7;font-weight:normal;">Muhammad Uzair Khattak,
                  </span>
              </div>

              <!-- Second Group of 2 Authors -->
              <div class="author-group">
                  <span class="author-block">
                      <a href="https://salman-h-khan.github.io/" style="color:#f68946;font-weight:normal;">Salman Khan</a>
                  </span>
              </div>
          </div>
            <div class="is-size-5 publication-authors">
              Mohamed bin Zayed University of Artificial Intelligence,<br>
            </div>
            <div class="is-size-5 publication-authors">
            Swiss Federal Institute of Technology Lausanne (EPFL), Australian National University<br>
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <span class="link-block">
                  <a href="https://www.arxiv.org/abs/2509.03986" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                  </a>
                </span>
                <span class="link-block">
                  <a href="https://github.com/insafim/Promptception" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

<!-- Teaser-->
<section class="section custom-section-teaser">
  <div class="columns is-centered has-text-centered">
      <div class="column is-half" style="display: flex; align-items: flex-start; justify-content: center;">
          <figure style="text-align: center;">
            <!-- <figcaption> -->
              <!-- <b>VideoGLaMM</b> is the first video-based Large Multimodal Model (LMM) with pixel-level grounding capabilities 🔥🔥🔥 -->
          <!-- </figcaption>   -->
            <img id="teaser" width="60%" src="static/images/sunburst.png">
              
          </figure>
      </div>
  </div>
</section>
<!--Teaser-->

  <!-- Abstract -->
  <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <h4 class="subtitle has-text-justified">
          Despite the success of Large Multimodal Models (LMMs) in recent years, prompt design for LMMs in Multiple-Choice Question Answering (MCQA) remains poorly understood. Ambiguity and Probabilistic Prompts We show that even minor variations in prompt phrasing and structure can lead to accuracy deviations of up to 15% for certain prompts and models. This variability poses a challenge for transparent and fair LMM evaluation, as models often report their best-case performance using carefully selected prompts. To address this, we introduce <b>Promptception</b>, a systematic framework for evaluating prompt sensitivity in LMMs. It consists of 61 prompt types, spanning 15 categories and 6 supercategories, each targeting specific aspects of prompt formulation, and is used to evaluate 10 LMMs ranging from lightweight open-source models to GPT-4o and Gemini 1.5 Pro, across 3 MCQA benchmarks: MMStar, MMMU-Pro, Roleplay Scenarios Choice Formatting and Presentation Structured Formatting Figure 1: Categorization of prompts proposed in our Promptception framework. It consists of 61 prompt types, spanning 15 categories (e.g. Answer Handling, Penalty-Based Prompts, Poor Linguistic Formatting) and 6 supercategories (e.g. TaREMOVED_KEYSpecific Instructions, MVBench. Our findings reveal that proprietary Focus-Driven Prompts Choice Formatting and Presentation), providing a commodels exhibit greater sensitivity to prompt phrasing, reflecting tighter alignment with instruction semantics, while open-source models are steadier but struggle with nuanced and complex phrasing. Based on this analysis, we propose Prompting Principles tailored to proprietary and open-source LMMs, enabling more robust and fair model evaluation.
        </h4>
      </div>
    </div>
  </section>


  <section class="section"  style="background-color:#efeff081">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-six-fifths">
        <h2 class="title is-3">🔥Highlights</h2>
        <div class="content has-text-justified">
            The contributions of this paper can be summarized as follows:

          <ol type="1">
            <li><b>Comprehensive Prompt Sensitivity Analysis:</b> We present the most extensive study to date on the impact of prompt variations across diverse multimodal benchmarks and LMM architectures. To facilitate this study, we introduce Promptception, a systematic evaluation framework comprising of 61 prompt types, organized into 15 categories and 6 supercategories, each designed to probe specific aspects of prompt formulation in LMMs.</li><br>
            <li><b>Evaluation Across Models, Modalities, and Benchmarks:</b> We assess prompt sensitivity across a diverse set of model sizes and architectures, including both open-source and proprietary LMMs. Our analysis spans multiple modalities and benchmarks; MMStar (single image), MMMU-Pro (multi-image), and MVBench (video) and we further evaluate sensitivity across various question dimensions within these benchmarks to ensure a comprehensive understanding.</li><br>
            <li><b>Best Practices for Prompting:</b> We identify key trends in prompting and propose Prompting Principles for effective and consistent evaluation of LMMs.</li><br>
          </ol>
        </div>
      </div>
    </div>
  </div>
</section>



<!--Model Arch-->
<section class="section">
  <div class="columns is-centered has-text-centered">
    <div class="column is-six-fifths">
      <h2 class="title is-3"><img src="static/images/Promptception-Logo.png" alt="Promptception_Logo" width="40" style="vertical-align: bottom;"> Sensitivity of state-of-the-art LMMs to prompt variations.</h2>
    </div>
  </div>

  <!-- <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <div class="content has-text-justified">
          <p>
            <b>VideoGLaMM</b> consists of
            following key components: (i) Spatio-Temporal Dual Encoder,
            (ii) Dual Alignment V-L Adapters for image and video fea-
            tures, (iii) Large Language Model (LLM) iv) L-V Adapter and (iv) Promptable
            Pixel Decoder.</p>
        </div>
      </div>
    </div> -->

    <div class="columns is-centered has-text-centered">
      <div class="column is-six-fifths" style="display: flex; align-items: flex-start; justify-content: center;">
        <figure style="text-align: center;">
          <img id="teaser" width="60%" src="static/images/fig2_comb.png">
          <figcaption>Examples from the MMStar benchmark
                      illustrating divergent model outputs despite identical user queries, caused solely by changes in prompt phrasing
                      (Left: InternVL-38B, Middle: GPT-4o, Right: Gemini 1.5 Pro). This demonstrates the models’ sensitivity to how
                      instructions are framed.
          </figcaption>
        </figure>
      </div>
    </div>
  </div>
</section>
<!-- Model Arch -->

<!-- Prompt Variation Impact -->
<section class="section" style="background-color:#f8f8f8;">
  <div class="container is-max-desktop">

    <!-- Title -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-six-fifths">
        <h2 class="title is-3">
          <img src="static/images/Promptception-Logo.png" alt="Promptception Logo" width="40" style="vertical-align: bottom;">
          How Does Variation in Prompts Impact Accuracy?
        </h2>
      </div>
    </div>

    <!-- Description -->
    <div class="content has-text-left" style="margin-bottom:20px;">
      <p>
        <b>Average Prompt Performance for Proprietary Models & Open-source Models.</b><br>
        PRA (Prompt Robustness Accuracy) with respect to the Baseline Prompt Accuracy is averaged across Proprietary Models 
        and the 3 Benchmarks (<i>MMStar, MMMU-Pro & MVBench</i>) for each Prompt Type.
      </p>
    </div>

    <!-- Figures stacked vertically -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-six-fifths">
        <figure style="text-align:center; margin-bottom:30px;">
          <img src="static/images/proprietary.png" alt="Proprietary Models Performance" class="figure-img" width="80%">
          <figcaption style="font-size:0.9em; margin-top:10px;">
            Performance variation across Proprietary LMMs
          </figcaption>
        </figure>

        <figure style="text-align:center;">
          <img src="static/images/open_source.png" alt="Open Source Models Performance" class="figure-img" width="80%">
          <figcaption style="font-size:0.9em; margin-top:10px;">
            Performance variation across Open-source LMMs
          </figcaption>
        </figure>
      </div>
    </div>

  </div>
</section>


<!-- Prompting Principles -->

<section class="section">
  <div class="container is-max-desktop">
    <!-- <h2 class="title is-3 has-text-centered">Prompting Principles</h2> -->
    <h2 class="title is-3 has-text-centered"><img src="static/images/Promptception-Logo.png" alt="Promptception_Logo" width="40" style="vertical-align: bottom;"> Prompting Principles</h2>
    <p class="content has-text-left">
      Based on the insights from our study, we outline best practices for optimizing LMM performance on the MCQA task. These strategies are designed to enhance both accuracy and consistency. While our insights are based on MCQA evaluations, we believe these principles can be broadly applied to other tasks and extended to LLMs and LMMs. An important observation underlying these principles is the clear difference in behavior between open-source and proprietary models. Open-source models are often not extensively instruction-tuned, which makes them less responsive to prompt variations. In contrast, proprietary models typically undergo rigorous instruction tuning with large-scale, high-quality data, as well as advanced reinforcement learning and post-training techniques. This makes them considerably more sensitive to user instructions, where even subtle changes in prompt phrasing can lead to notable differences in performance. Given these differences in instruction-following capabilities, we present prompting principles separately for open-source and proprietary models. This distinction allows us to account for their varying adherence to instructions and to highlight strategies that are most effective for each category.
    </p>
    <br/>

    <div style="overflow-x:auto;">
      <table class="table is-bordered is-striped is-hoverable is-fullwidth">
        <thead>
          <tr>
            <th style="text-align:center; width:3%;">#</th>
            <th style="text-align:center;">Open-Source Models</th>
            <th style="text-align:center;">Proprietary Models</th>
          </tr>
        </thead>
        <tbody>

          <!-- Row 1 -->
          <tr>
            <td style="text-align:center;">1</td>
            <td>
              <b>Concise prompts yield better performance</b>: Short and direct prompts improve accuracy. 
              <i>"Answer with the option letter from the given choices directly."</i> <span style="color:blue;">(1.1)</span>
              <hr>
              <b>Overly short or vague prompts reduce accuracy</b>: Too brief and unclear. 
              <i>"Best Choice: $LETTER"</i> <span style="color:blue;">(12.3)</span>
              <hr>
              <b>Detailed prompts are ineffective</b>: Long or highly descriptive prompts do not help. 
              <span style="color:blue;">(Category 5, etc.)</span>
            </td>
            <td>
              <b>Prompt length and detail have minimal impact</b>: Stable across lengths. <br><hr>
              <b>Restricting responses to only a letter is detrimental</b>: Suppresses reasoning and reduces accuracy. 
              <span style="color:blue;">(12.2)</span>
            </td>
          </tr>

          <!-- Row 2 -->
          <tr>
            <td style="text-align:center;">2</td>
            <td>
              <b>Complex or structured formatting decreases accuracy</b>: JSON, YAML, Markdown harm results. 
              <span style="color:blue;">(2.3–2.9)</span>
              <hr>
              <b>Clear separation of option letters enhances clarity</b>: e.g., 
              <i>(A) choice1 <br>(B) choice2 <br>(C) choice3 <br>(D) choice4</i> 
              <span style="color:blue;">(1.2)</span>
              <hr>
              <b>Explicit labeling of Q&A is beneficial</b>: 
              <i>“Question: … Options: … Answer …”</i> <span style="color:blue;">(2.2)</span>
              <hr>
              <b>Placing Q&A at the end helps</b>. <span style="color:blue;">(3.1)</span>
            </td>
            <td>
              <b>Complex formatting does not impair accuracy</b>: Proprietary models handle JSON/Markdown/YAML fine. 
              <span style="color:blue;">(Category 2)</span>
            </td>
          </tr>

          <!-- Row 3 -->
          <tr>
            <td style="text-align:center;">3</td>
            <td>
              <b>Poor linguistic formatting hinders performance</b>: ALL CAPS, grammar issues, misspellings reduce accuracy. 
              <span style="color:blue;">(Category 4)</span>
            </td>
            <td>
              <b>Poor linguistic formatting does not affect performance</b>: Proprietary models are robust to casing/typos. 
              <span style="color:blue;">(Category 4)</span>
            </td>
          </tr>

          <!-- Row 4 -->
          <tr>
            <td style="text-align:center;">4</td>
            <td>
              <b>Chain-of-Thought reasoning is ineffective</b>: Step-by-step reasoning does not improve accuracy. 
              <span style="color:blue;">(Category 6)</span>
            </td>
            <td>
              <b>Allowing reasoning improves accuracy</b>: Letting model “think” yields higher accuracy. 
              <span style="color:blue;">(Categories 6 & 12.5)</span>
            </td>
          </tr>

          <!-- Row 5 -->
          <tr>
            <td style="text-align:center;">5</td>
            <td>
              <b>Penalties, incentives, or competitive framing are ineffective</b>: Rewards/penalties add ambiguity. 
              <span style="color:blue;">(Categories 13–15)</span>
            </td>
            <td>
              <b>Penalties or incentives improve performance</b>: Contextual framing helps. 
              <span style="color:blue;">(Categories 13 & 14)</span>
              <hr>
              <b>Competitive framing degrades performance</b>: Game-like prompts reduce accuracy. 
              <span style="color:blue;">(Category 15)</span>
            </td>
          </tr>

          <!-- Row 6 -->
          <tr>
            <td style="text-align:center;">6</td>
            <td>
              <b>Specifying personas/audiences is ineffective</b>: No performance gain. 
              <span style="color:blue;">(Categories 8 & 9)</span>
            </td>
            <td>
              <b>Persona-based prompting has mixed effects</b>: Positive neutral, negative harmful. 
              <span style="color:blue;">(Category 9)</span>
            </td>
          </tr>

          <!-- Row 7 -->
          <tr>
            <td style="text-align:center;">7</td>
            <td>
              <b>Overemphasis on answer format is unhelpful</b>: Excessive formatting instructions degrade accuracy. 
              <span style="color:blue;">(Categories 11.3 & 12)</span>
            </td>
            <td>
              <b>Answer format matters</b>: Proprietary models are sensitive to requested format. 
              <span style="color:blue;">(Categories 11.3 & 12)</span>
            </td>
          </tr>

          <!-- Row 8 -->
          <tr>
            <td style="text-align:center;">8</td>
            <td>
              <b>Temporal reasoning enhances video comprehension</b>: Improves accuracy. 
              <span style="color:blue;">(11.4, 11.5)</span>
            </td>
            <td>
              <b>Temporal reasoning enhances video comprehension</b>: Same benefit for proprietary models. 
              <span style="color:blue;">(11.4 & 11.5)</span>
            </td>
          </tr>

          <!-- Row 9 -->
          <tr>
            <td style="text-align:center;">9</td>
            <td>
              <b>Image-focused prompting helps</b>: Direct attention to image improves accuracy. 
              <span style="color:blue;">(11.1)</span>
            </td>
            <td>
              <b>Focusing only on image/question hinders</b>: Explicit restriction worsens performance. 
              <span style="color:blue;">(11.1 & 11.2)</span>
            </td>
          </tr>

          <!-- Row 10 -->
          <tr>
            <td style="text-align:center;">10</td>
            <td>
              <b>Answer leakage degrades performance</b>: Unintended hints reduce accuracy. 
              <span style="color:blue;">(Category 7)</span>
            </td>
            <td>
              <b>Asking to avoid bias/stereotypes helps</b>: Improves responses. 
              <span style="color:blue;">(Category 10)</span>
            </td>
          </tr>

        </tbody>
      </table>
    </div>
  </div>
</section>

<!-- BibTeX -->
<style>
  #BibTeX {
    margin-bottom: -80px; /* Adjust the negative margin as needed */
  }
  #Acknowledgement {
    margin-top: -80px; /* Adjust the negative margin as needed */
  }
</style>

<!-- BibTex -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>
@misc{ismithdeen2025promptceptionsensitivelargemultimodal,
      title={Promptception: How Sensitive Are Large Multimodal Models to Prompts?}, 
      author={Mohamed Insaf Ismithdeen and Muhammad Uzair Khattak and Salman Khan},
      year={2025},
      eprint={2509.03986},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2509.03986}, 
}
  </code></pre>
    </div>
  </section>
 
  <!-- Acknowledgement -->
  <section class="section" id="Acknowledgement">
    <div class="container is-max-desktop content">
      <h2 class="title">Acknowledgement</h2>
      <p>
        This website is adapted from <a
        href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>, licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
        Commons Attribution-ShareAlike 4.0 International License</a>.
      </p>
    </div>
  </section>

</body>

</html>

<!-- <div style="text-align: center;">
  <a href="https://www.ival-mbzuai.com" target="_blank">
    <img src="images/logos/IVAL_logo.png" width="200" height="100" alt="IVAL Logo">
  </a>
  <a href="https://github.com/mbzuai-oryx" target="_blank">
    <img src="images/logos/Oryx_logo.png" width="100" height="100" alt="Oryx Logo">
  </a>
  <a href="https://mbzuai.ac.ae" target="_blank">
    <img src="static/images/MBZUAI_logo.png" width="360" height="85" alt="MBZUAI Logo">
  </a>
</div> -->

<div style="text-align: center;">
  <a href="https://mbzuai.ac.ae" target="_blank">
    <img src="static/images/MBZUAI_logo.png" width="360" height="85" alt="MBZUAI Logo">
  </a>
</div>